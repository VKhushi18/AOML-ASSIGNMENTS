{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51ea9c7-78bb-4e73-b9ab-dd9eb6e2f98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "    Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       " 0            6      148             72             35        0  33.6   \n",
       " 1            1       85             66             29        0  26.6   \n",
       " 2            8      183             64              0        0  23.3   \n",
       " 3            1       89             66             23       94  28.1   \n",
       " 4            0      137             40             35      168  43.1   \n",
       " \n",
       "    DiabetesPedigreeFunction  Age  Outcome  \n",
       " 0                     0.627   50        1  \n",
       " 1                     0.351   31        0  \n",
       " 2                     0.672   32        1  \n",
       " 3                     0.167   21        0  \n",
       " 4                     2.288   33        1  )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"diabetes.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.info(), df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47aa5466-20a6-4c3f-8b6c-eb799bc81a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BMI_category\n",
       "Obese            478\n",
       "Overweight       174\n",
       "Normal weight    101\n",
       "Underweight       15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define BMI categories based on standard medical guidelines\n",
    "def categorize_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return \"Underweight\"\n",
    "    elif 18.5 <= bmi < 24.9:\n",
    "        return \"Normal weight\"\n",
    "    elif 25 <= bmi < 29.9:\n",
    "        return \"Overweight\"\n",
    "    else:\n",
    "        return \"Obese\"\n",
    "\n",
    "# Apply the function to create the new column\n",
    "df[\"BMI_category\"] = df[\"BMI\"].apply(categorize_bmi)\n",
    "\n",
    "# Display value counts to verify\n",
    "df[\"BMI_category\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98031ee0-6cee-4626-b865-a994fad87ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((614, 9), (154, 9), (614,), (154,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=[\"Outcome\"])  # Features\n",
    "y = df[\"Outcome\"]  # Target variable\n",
    "\n",
    "# Split data into training (80%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Display the shape of the resulting datasets\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape\n",
    "\n",
    "##The dataset has been split into training (614 samples) and validation (154 samples) sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574f7ee1-9254-483a-9c79-857f35f4de76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((614, 12), (154, 12))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "categorical_features = [\"BMI_category\"]\n",
    "\n",
    "# Create column transformer with StandardScaler and OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the training set\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation set\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "\n",
    "# Display transformed feature shape\n",
    "X_train_transformed.shape, X_val_transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b78b1c5-748b-432a-b3a6-81e6a05bd2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.5882352941176471, 5, 0.7037037037037037)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Experimenting with different values of k for KNN\n",
    "knn_scores = {}\n",
    "for k in [3, 5, 7]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_transformed, y_train)\n",
    "    y_pred = knn.predict(X_val_transformed)\n",
    "    knn_scores[k] = f1_score(y_val, y_pred)\n",
    "\n",
    "# Select the best k value\n",
    "best_k = max(knn_scores, key=knn_scores.get)\n",
    "best_knn_f1 = knn_scores[best_k]\n",
    "\n",
    "# Experimenting with different values of max_depth for Decision Tree\n",
    "dt_scores = {}\n",
    "for depth in [3, 5, 7]:\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train_transformed, y_train)\n",
    "    y_pred = dt.predict(X_val_transformed)\n",
    "    dt_scores[depth] = f1_score(y_val, y_pred)\n",
    "\n",
    "# Select the best max_depth value\n",
    "best_depth = max(dt_scores, key=dt_scores.get)\n",
    "best_dt_f1 = dt_scores[best_depth]\n",
    "\n",
    "best_k, best_knn_f1, best_depth, best_dt_f1\n",
    "\n",
    "\n",
    "#The best hyperparameters based on F1 score are:\n",
    "\n",
    "##KNN: K=5 with an F1 score of 0.588.\n",
    "##Decision Tree: max_depth=5 with an F1 score of 0.704.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fd12bba-bcf0-4858-a190-ca32310a5e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved successfully in: ./saved_models/\n",
      "Saved files: ['scaler.pkl', 'encoder.pkl', 'best_model.pkl']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define a writable directory\n",
    "save_dir = \"./saved_models/\"  # Ensure this directory exists\n",
    "scaler_path = os.path.join(save_dir, \"scaler.pkl\")\n",
    "encoder_path = os.path.join(save_dir, \"encoder.pkl\")\n",
    "model_path = os.path.join(save_dir, \"best_model.pkl\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save StandardScaler and OneHotEncoder separately\n",
    "joblib.dump(preprocessor.named_transformers_[\"num\"], scaler_path)  # Save StandardScaler\n",
    "joblib.dump(preprocessor.named_transformers_[\"cat\"], encoder_path)  # Save OneHotEncoder\n",
    "\n",
    "# Save the best Decision Tree model\n",
    "joblib.dump(dt, model_path)\n",
    "\n",
    "# Confirm that files are saved correctly\n",
    "print(\"Files saved successfully in:\", save_dir)\n",
    "print(\"Saved files:\", os.listdir(save_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2cb1d72-e440-44e1-8364-6da0e3e41ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the correct path where files were saved\n",
    "save_dir = \"./saved_models/\"\n",
    "\n",
    "# Ensure files exist before loading\n",
    "if not os.path.exists(os.path.join(save_dir, \"scaler.pkl\")):\n",
    "    raise FileNotFoundError(\"scaler.pkl not found in the expected directory.\")\n",
    "if not os.path.exists(os.path.join(save_dir, \"best_model.pkl\")):\n",
    "    raise FileNotFoundError(\"best_model.pkl not found in the expected directory.\")\n",
    "\n",
    "# Load the saved preprocessor and model\n",
    "scaler = joblib.load(os.path.join(save_dir, \"scaler.pkl\"))\n",
    "encoder = joblib.load(os.path.join(save_dir, \"encoder.pkl\"))\n",
    "model = joblib.load(os.path.join(save_dir, \"best_model.pkl\"))\n",
    "\n",
    "print(\"Files loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89a57b35-9bbc-4afb-9a54-a3fcbd36a34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Predicted Outcome = 0\n",
      "Sample 2: Predicted Outcome = 1\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define correct file paths\n",
    "save_dir = \"./saved_models/\"  # Update this to the correct path if needed\n",
    "\n",
    "# Load the saved preprocessor and model\n",
    "scaler = joblib.load(os.path.join(save_dir, \"scaler.pkl\"))\n",
    "encoder = joblib.load(os.path.join(save_dir, \"encoder.pkl\"))  # Load OneHotEncoder\n",
    "model = joblib.load(os.path.join(save_dir, \"best_model.pkl\"))\n",
    "\n",
    "# Define numerical and categorical columns (same as training)\n",
    "num_features = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "cat_features = [\"BMI_category\"]\n",
    "\n",
    "def preprocess(sample):\n",
    "    \"\"\"\n",
    "    Function to preprocess a sample before prediction.\n",
    "    :param sample: Dictionary containing feature values\n",
    "    :return: Transformed sample (ready for model prediction)\n",
    "    \"\"\"\n",
    "    # Convert sample to DataFrame\n",
    "    sample_df = pd.DataFrame([sample])\n",
    "\n",
    "    # Apply OneHotEncoder transformation to categorical features\n",
    "    sample_cat = encoder.transform(sample_df[cat_features]).toarray()\n",
    "\n",
    "    # Apply StandardScaler to numerical features\n",
    "    sample_num = scaler.transform(sample_df[num_features])\n",
    "\n",
    "    # Concatenate numerical and encoded categorical features\n",
    "    sample_transformed = np.hstack((sample_num, sample_cat))\n",
    "\n",
    "    return sample_transformed\n",
    "\n",
    "def predict(sample):\n",
    "    \"\"\"\n",
    "    Function to preprocess and predict the outcome for a given sample.\n",
    "    :param sample: Dictionary containing feature values\n",
    "    :return: Predicted class (0 or 1)\n",
    "    \"\"\"\n",
    "    sample_transformed = preprocess(sample)\n",
    "    prediction = model.predict(sample_transformed)\n",
    "    return prediction[0]\n",
    "\n",
    "# Example test cases from validation set\n",
    "test_samples = [\n",
    "    {\n",
    "        \"Pregnancies\": 2,\n",
    "        \"Glucose\": 120,\n",
    "        \"BloodPressure\": 70,\n",
    "        \"SkinThickness\": 20,\n",
    "        \"Insulin\": 79,\n",
    "        \"BMI\": 25.5,\n",
    "        \"DiabetesPedigreeFunction\": 0.45,\n",
    "        \"Age\": 30,\n",
    "        \"BMI_category\": \"Overweight\"\n",
    "    },\n",
    "    {\n",
    "        \"Pregnancies\": 5,\n",
    "        \"Glucose\": 140,\n",
    "        \"BloodPressure\": 80,\n",
    "        \"SkinThickness\": 35,\n",
    "        \"Insulin\": 90,\n",
    "        \"BMI\": 29.0,\n",
    "        \"DiabetesPedigreeFunction\": 0.55,\n",
    "        \"Age\": 45,\n",
    "        \"BMI_category\": \"Overweight\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run predictions\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"Sample {i+1}: Predicted Outcome =\", predict(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae920d8-f3a0-4898-8860-c804f23e6262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
